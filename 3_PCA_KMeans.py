# -*- coding: utf-8 -*-
"""final_PCA_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8T2NSHpMwxNmZ4_grsdjgrFVPgL7Rve
"""

from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA, LatentDirichletAllocation, NMF, FactorAnalysis
from sklearn.cluster import KMeans, SpectralClustering, Birch
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from sklearn.covariance import GraphicalLasso
from sklearn.metrics import mean_squared_error, silhouette_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from time import time

with open('data.pickle', 'rb') as handle:
    subs_train, subs_test, subs_val, hlth_train, hlth_test, hlth_val, demo_train, demo_test, demo_val = pickle.load(handle)

subs_train_o = subs_train.drop(['QUESTID2', 'FILEDATE'], axis=1)
subs_test_o = subs_test.drop(['QUESTID2', 'FILEDATE'], axis=1)
subs_val_o = subs_val.drop(['QUESTID2', 'FILEDATE'], axis=1)

with open('data_removed_updated.pickle', 'rb') as handle:
    all_train, all_test, subs_train, subs_test, subs_val, hlth_train, hlth_test, hlth_val, demo_train, demo_test, demo_val = pickle.load(handle)

all_train = pd.concat([subs_train_o, hlth_train, demo_train], axis=1)
all_test = pd.concat([subs_test_o, hlth_test, demo_test], axis=1)
all_val = pd.concat([subs_val_o, hlth_val, demo_val], axis=1)

all_test = pd.concat([all_test, all_val], axis=0)

# Remove features with greater than 10 unique values
for feature in all_train.keys():
    if all_train[feature].nunique() > 10:
        all_train = all_train.drop([feature], axis=1)
        all_test = all_test.drop([feature], axis=1)

for feature in subs_train_o.keys():
    if subs_train_o[feature].nunique() > 10:
        subs_train_o = subs_train_o.drop([feature], axis=1)
        subs_test_o = subs_test_o.drop([feature], axis=1)

for feature in hlth_train.keys():
    if hlth_train[feature].nunique() > 10:
        hlth_train = hlth_train.drop([feature], axis=1)
        hlth_test = hlth_test.drop([feature], axis=1)

for feature in demo_train.keys():
    if demo_train[feature].nunique() > 10:
        demo_train = demo_train.drop([feature], axis=1)
        demo_test = demo_test.drop([feature], axis=1)

profile_train = pd.get_dummies(all_train, columns=all_train.columns)
profile_test = pd.get_dummies(all_test, columns=all_train.columns)

# Remove features that are not present in both train and test sets
common = np.intersect1d(profile_train.columns, profile_test.columns)
profile_train = profile_train[common]
profile_test = profile_test[common]

# Get dummies for each dataset because they are categorical and KMeans requires numerical input
profile_subs = pd.get_dummies(subs_train_o, columns=subs_train_o.columns)
profile_subs_test = pd.get_dummies(subs_test_o, columns=subs_train_o.columns)
profile_hlth = pd.get_dummies(hlth_train, columns=hlth_train.columns)
profile_hlth_test = pd.get_dummies(hlth_test, columns=hlth_train.columns)
profile_demo = pd.get_dummies(demo_train, columns=demo_train.columns)
profile_demo_test = pd.get_dummies(demo_test, columns=demo_train.columns)

# Remove features that are not present in both train and test sets
common = np.intersect1d(profile_subs.columns, profile_subs_test.columns)
profile_subs = profile_subs[common]
profile_subs_test = profile_subs_test[common]
common = np.intersect1d(profile_hlth.columns, profile_hlth_test.columns)
profile_hlth = profile_hlth[common]
profile_hlth_test = profile_hlth_test[common]
common = np.intersect1d(profile_demo.columns, profile_demo_test.columns)
profile_demo = profile_demo[common]
profile_demo_test = profile_demo_test[common]

profile_subs_demo = pd.concat([profile_subs, profile_demo], axis=1)
profile_subs_hlth = pd.concat([profile_subs, profile_hlth], axis=1)

print(f"all_train: Before: {all_train.shape}     After: {profile_train.shape}")
print(f"all_test: Before: {all_test.shape}     After: {profile_test.shape}")
print(f"subs_train_o: Before: {subs_train_o.shape}     After: {profile_subs.shape}")
print(f"hlth_train: Before: {hlth_train.shape}     After: {profile_hlth.shape}")
print(f"demo_train: Before: {demo_train.shape}     After: {profile_demo.shape}")
print(f"subs_demo: {str(profile_subs_demo.shape)}")
print(f"subs_hlth: {str(profile_subs_hlth.shape)}")

train = profile_train
test = profile_test

# ----------- PCA parameter tuning -----------
'''
fig = plt.figure(figsize=(14, 10))
hrange = np.arange(2, 40, 2)
LL = np.zeros(len(hrange))
RE = np.zeros(len(hrange))
for idx in range(len(hrange)):
    model = PCA(n_components=hrange[idx])
    model.fit(train.values)
    train_transformed = model.transform(train.values)
    LL[idx] = model.score(train.values) / train.shape[0]
    train_origin = model.inverse_transform(train_transformed)
    RE[idx] = ((train.values - train_origin) ** 2).mean()

ax1 = fig.add_subplot(2, 3, 1)
pca = PCA().fit(train.values)
ax1.plot(np.cumsum(pca.explained_variance_ratio_)[2:40], 'bx-')
ax1.set_xlabel('number of components')
ax1.set_ylabel('cumulative variance', fontsize=9)

ax2 = fig.add_subplot(2, 3, 2)
ax2.plot(hrange, LL, 'bx-', label="LL")
ax2.set_xlabel('The number of components')
ax2.set_ylabel('LL', fontsize=9)

ax3 = fig.add_subplot(2, 3, 3)
ax3.plot(hrange, RE, 'bx-', label="RE")
ax3.set_xlabel('The number of components')
ax3.set_ylabel('RE', fontsize=9)
'''

def analyze_pca_metrics(data, name, hrange=np.arange(2, 40, 2)):
    LL = np.zeros(len(hrange))
    RE = np.zeros(len(hrange))

    fig = plt.figure(figsize=(15, 5))

    # Calculate LL and RE for different components
    for idx in range(len(hrange)):
        model = PCA(n_components=hrange[idx])
        model.fit(data.values)
        data_transformed = model.transform(data.values)
        LL[idx] = model.score(data.values) / data.shape[0]
        data_origin = model.inverse_transform(data_transformed)
        RE[idx] = ((data.values - data_origin) ** 2).mean()

    # Plot results
    ax1 = fig.add_subplot(1, 3, 1)
    pca = PCA().fit(data.values)
    ax1.plot(np.cumsum(pca.explained_variance_ratio_)[2:40], 'bx-')
    ax1.set_xlabel('number of components')
    ax1.set_ylabel('cumulative variance')
    ax1.set_title(f'{name} - Cumulative Variance')

    ax2 = fig.add_subplot(1, 3, 2)
    ax2.plot(hrange, LL, 'bx-', label="LL")
    ax2.set_xlabel('number of components')
    ax2.set_ylabel('ALL')
    ax2.set_title(f'{name} - Log Likelihood')

    ax3 = fig.add_subplot(1, 3, 3)
    ax3.plot(hrange, RE, 'bx-', label="RE")
    ax3.set_xlabel('number of components')
    ax3.set_ylabel('RE')
    ax3.set_title(f'{name} - Reconstruction Error')

    plt.tight_layout()
    plt.show()

    return LL, RE

#calculate metrics for all datasets separately
subs_LL, subs_RE = analyze_pca_metrics(profile_subs, "SUBSTANCES")
hlth_LL, hlth_RE = analyze_pca_metrics(profile_hlth, "HEALTH")
demo_LL, demo_RE = analyze_pca_metrics(profile_demo, "DEMOGRAPHY")
subs_hlth_LL, subs_hlth_RE = analyze_pca_metrics(profile_subs_hlth, "SUBSTANCES_HEALTH")
subs_demo_LL, subs_demo_RE = analyze_pca_metrics(profile_subs_demo, "SUBSTANCES_DEMO")
train_LL, train_RE = analyze_pca_metrics(train, "TRAIN")

# Compare optimal components for each dataset using RE
hrange = np.arange(2, 40, 1)

def find_optimal_components(RE, hrange):
    differences = np.diff(RE)
    mean_diff = np.mean(np.abs(differences))
    #smallest difference gives flat part of the curve
    first_small_diff = np.where(np.abs(differences) < mean_diff)[0][0]
    return hrange[first_small_diff + 1]

print("Optimal components:")
print(f"Substance Use: {find_optimal_components(subs_RE, hrange)}")
print(f"Health: {find_optimal_components(hlth_RE, hrange)}")
print(f"Demographics: {find_optimal_components(demo_RE, hrange)}")
print(f"Substances_Health: {find_optimal_components(subs_hlth_RE, hrange)}")
print(f"Substances_Demographics: {find_optimal_components(subs_demo_RE, hrange)}")
print(f"Combined: {find_optimal_components(train_RE, hrange)}")

# ----------- K-mean parameter tuning -----------
model = PCA(n_components=8)
model.fit(train.values)
train_transformed = model.transform(train.values)

Sum_of_squared_distances = []
silhouette_avg = []
score = []
K = range(2, 40, 2)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(train_transformed)
    Sum_of_squared_distances.append(km.inertia_)
    cluster_labels = km.predict(train_transformed)
    silhouette_avg.append(silhouette_score(train_transformed, cluster_labels))
    score.append(km.score(train_transformed))

fig = plt.figure(figsize=(15, 5))

ax1 = fig.add_subplot(1, 3, 1)
ax1.plot(K, Sum_of_squared_distances, 'bx-')
ax1.set_xlabel('The number of clusters')
ax1.set_ylabel('sum of dist^2', fontsize=9)

ax2 = fig.add_subplot(1, 3, 2)
ax2.plot(K, silhouette_avg, 'bx-')
ax2.set_xlabel('The number of clusters')
ax2.set_ylabel('silhouette', fontsize=9)

ax3 = fig.add_subplot(1, 3, 3)
ax3.plot(K, score, 'bx-')
ax3.set_xlabel('The number of clusters')
ax3.set_ylabel('score', fontsize=9)

fig.tight_layout()
out_data = "./FA_Kmeans_param.png"
fig.savefig(out_data)

# ---------------------- DATA TYPE/CLUSTER ----------------------
models = [
    {
        'name': 'DRUG',
        'function': PCA(n_components=8),
        'train': profile_subs
    },
    {
        'name': 'HEALTH',
        'function': PCA(n_components=8),
        'train': profile_hlth
    },
    {
        'name': 'DEMOGRAPHY',
        'function': PCA(n_components=8),
        'train': profile_demo
    },
    {
        'name': 'SUBSTANCES_HEALTH',
        'function': PCA(n_components=8),
        'train': profile_subs_hlth
    },
    {
        'name': 'SUBSTANCES_DEMO',
        'function': PCA(n_components=8),
        'train': profile_subs_demo
    },
    {
        'name': 'ALL',
        'function': PCA(n_components=8),
        'train': profile_train
    }
]

dataset_labels = ['DRUG', 'HEALTH', 'DEMOGRAPHY', 'SUBSTANCES_HEALTH', 'SUBSTANCES_DEMO', 'ALL']

grid = 6
fig2 = plt.figure(figsize=(14, 10))

for n_idx, n in enumerate(models):
    print('\n ----------GRAPH GENERATION ' + n['name'] + '----------')

    model = n['function']
    train = n['train']
    print(train.shape)

    model.fit(train.values)
    train_transformed = model.transform(train.values)

    targets = [1, 2]
    colors = ['r.', 'b.']
    kmeans = KMeans(n_clusters=10).fit(train_transformed)
    pred_target = kmeans.predict(train_transformed)

    # Plot clusters (changed from 12 to 11)
    for cluster_num in range(10):
        plt3 = fig2.add_subplot(grid, 11, 1 + 11 * n_idx + cluster_num)  # Changed from 12 to 11
        for target, color in zip(targets, colors):
            target_idx = (pred_target == cluster_num)
            target_idx2 = all_train['IRSEX'] == target
            idx = target_idx & target_idx2
            plt3.plot(train_transformed[idx, 0], train_transformed[idx, 1], color, markersize=1)

        if cluster_num == 0:
            plt3.set_ylabel(dataset_labels[n_idx], fontsize=10)

        plt.setp(plt3.get_xticklabels(), visible=False)
        plt.setp(plt3.get_yticklabels(), visible=False)

    # Plot gender distribution (changed from 12 to 11)
    plt1 = fig2.add_subplot(grid, 11, 11 + 11 * n_idx)  # Changed from 12 to 11
    for target, color in zip(targets, colors):
        target_idx2 = all_train['IRSEX'] == target
        plt1.plot(train_transformed[target_idx2, 0], train_transformed[target_idx2, 1],
                 color, markersize=1, label='Male' if target==1 else 'Female')

    # Add legend only to the last row
    if n_idx == len(models)-1:
        plt1.legend(bbox_to_anchor=(1.05, 0.5))

    plt.setp(plt1.get_xticklabels(), visible=False)
    plt.setp(plt1.get_yticklabels(), visible=False)

    '''
    plt1 = fig2.add_subplot(grid, 12, 12 + 12 * n_idx)
    plt.setp(plt1.get_xticklabels(), visible=False)
    plt.setp(plt1.get_yticklabels(), visible=False)
    plt.legend()
    '''
    latent_users = model.components_
    for user_num in range(8):
        print('\n -----USER ' + str(user_num) + '-----')

        # what features affects the most
        lu = latent_users[user_num, :]
        # print("Latent user shape:", latent_users.shape)
        up = train_transformed[:, user_num]
        # print("User proportions from latent user shape:", up.shape)
        lu_indices = np.where(lu > np.percentile(lu, 99))[0]
        print(lu_indices)
        top_lu_df = pd.DataFrame({'Feature': train.columns.values[lu_indices],
                                   'Latent User Pseudocount': lu[lu_indices]})
        top_lu_df.sort_values('Latent User Pseudocount', inplace=True, ascending=False)
        # print(top_lu_df.head(n=5))

        # what users are highest in component
        tmp_profile_df = train.copy()
        tmp_profile_df.insert(0, "User Proportion from Latent User", up)
        tmp_profile_df.sort_values('User Proportion from Latent User', inplace=True, ascending=False)
        # print(tmp_profile_df.head())
        # train.loc[44412, 'ALCYFU']

    n['train_LL'] = model.score(train.values)
    print("Train - Log-likelihood: ", n['train_LL'])
    n['train_ALL'] = model.score(train.values) / train.shape[0]  # not working when num_samp < num_feature
    print("Train - Average Log-Likelihood: ", n['train_ALL'])
    train_origin = model.inverse_transform(train_transformed)
    n['train_RE'] = ((train.values - train_origin) ** 2).mean()
    print("Train - Reconstruction Error: ", n['train_RE'])

    centroids = kmeans.cluster_centers_
    print("Train - Average Log-Likelihood with Kmeans: ", kmeans.score(train_transformed) / train.shape[0])
    print("Train - sum of squared distance: ", kmeans.inertia_)
    cluster_labels = kmeans.predict(train_transformed)
    print("Train - silhouette: ", silhouette_score(train_transformed, cluster_labels))

    for target, color in zip(targets, colors):
        print(target)
        target_idx = all_train['IRSEX'] == target
        cluster_labels = kmeans.predict(train_transformed[target_idx, :])
        print("Train - silhouette: ", silhouette_score(train_transformed[target_idx, :], cluster_labels))

out_data = "./UP.png"
fig2.tight_layout()
fig2.savefig(out_data, bbox_inches='tight')

# Define all datasets to analyze
datasets = [
    {'name': 'ALL', 'train': train, 'test': test},
    {'name': 'DRUG', 'train': subs_train, 'test': subs_test},
    {'name': 'HEALTH', 'train': hlth_train, 'test': hlth_test},
    {'name': 'DEMO', 'train': demo_train, 'test': demo_test}
]

for dataset in datasets:
    print(f"\n----- Analysis for {dataset['name']} Dataset -----")

    # Apply PCA
    pca = PCA(n_components=8)
    pca.fit(dataset['train'].values)

    # Transform both train and test data
    train_transformed = pca.transform(dataset['train'].values)
    test_transformed = pca.transform(dataset['test'].values)

    # Calculate explained variance
    explained_var = pca.explained_variance_ratio_
    print(f"Explained variance ratio: {explained_var}")
    print(f"Cumulative explained variance: {np.cumsum(explained_var)}")

    # Perform K-means clustering
    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(train_transformed)

    # Get cluster predictions for both train and test
    train_labels = kmeans.labels_
    test_labels = kmeans.predict(test_transformed)

    # Calculate metrics for both train and test
    train_silhouette = silhouette_score(train_transformed, train_labels)
    test_silhouette = silhouette_score(test_transformed, test_labels)
    train_score = kmeans.score(train_transformed) / dataset['train'].shape[0]
    test_score = kmeans.score(test_transformed) / dataset['test'].shape[0]

    print(f"Train Silhouette Score: {train_silhouette:.3f}")
    print(f"Test Silhouette Score: {test_silhouette:.3f}")
    print(f"Train Average Log-Likelihood: {train_score:.3f}")
    print(f"Test Average Log-Likelihood: {test_score:.3f}")

    # Create subplot for train and test
    plt.figure(figsize=(20, 8))

    # Plot training data
    plt.subplot(1, 2, 1)
    scatter1 = plt.scatter(
        train_transformed[:, 0],
        train_transformed[:, 1],
        c=train_labels,
        cmap='Set1',
        alpha=0.6,
        s=10
    )
    plt.scatter(
        kmeans.cluster_centers_[:, 0],
        kmeans.cluster_centers_[:, 1],
        marker='X',
        s=200,
        linewidths=3,
        color='k',
        label='Centroids'
    )
    plt.title(f'Training Data Clusters\nSilhouette: {train_silhouette:.3f}')
    plt.xlabel('First Principal Component')
    plt.ylabel('Second Principal Component')
    plt.legend()
    plt.colorbar(scatter1, label='Cluster')

    # Plot test data
    plt.subplot(1, 2, 2)
    scatter2 = plt.scatter(
        test_transformed[:, 0],
        test_transformed[:, 1],
        c=test_labels,
        cmap='Set1',
        alpha=0.6,
        s=10
    )
    plt.scatter(
        kmeans.cluster_centers_[:, 0],
        kmeans.cluster_centers_[:, 1],
        marker='X',
        s=200,
        linewidths=3,
        color='k',
        label='Centroids'
    )
    plt.title(f'Test Data Clusters\nSilhouette: {test_silhouette:.3f}')
    plt.xlabel('First Principal Component')
    plt.ylabel('Second Principal Component')
    plt.legend()
    plt.colorbar(scatter2, label='Cluster')

    plt.suptitle(f'K-means Clustering on {dataset["name"]} Dataset', fontsize=16)
    plt.tight_layout()

    # Save the plot
    plt.savefig(f'KMeans_Cluster_{dataset["name"]}_Train_Test.png')

    # Display the plot
    plt.show()

    # Close the figure to free memory
    plt.close()

    # Analyze cluster characteristics
    print("\nCluster Characteristics:")
    for i, centroid in enumerate(kmeans.cluster_centers_):
        print(f"\nCluster {i}:")
        print("PCA components:", centroid)

        # Calculate cluster sizes for both train and test
        train_size = np.sum(train_labels == i)
        test_size = np.sum(test_labels == i)
        print(f"Train samples in cluster: {train_size} ({train_size/len(train_labels)*100:.1f}%)")
        print(f"Test samples in cluster: {test_size} ({test_size/len(test_labels)*100:.1f}%)")

        # Get top features for the most significant PCA components
        top_pc_indices = np.argsort(-np.abs(centroid))[:3]
        print(f"Most important PCA components: PC{top_pc_indices[0]+1}, PC{top_pc_indices[1]+1}, PC{top_pc_indices[2]+1}")

# Define all datasets to analyze
datasets = [
    {'name': 'ALL', 'train': train, 'test': test},
    {'name': 'DRUG', 'train': subs_train, 'test': subs_test},
    {'name': 'HEALTH', 'train': hlth_train, 'test': hlth_test},
    {'name': 'DEMO', 'train': demo_train, 'test': demo_test}
]

for dataset in datasets:
    print(f"\n{'='*50}")
    print(f"Analysis for {dataset['name']} Dataset")
    print(f"{'='*50}")

    # Apply PCA
    pca = PCA(n_components=8)
    pca.fit(dataset['train'].values)

    # Get feature names
    feature_names = dataset['train'].columns

    # Create DataFrame of PCA loadings
    pca_loadings = pd.DataFrame(
        pca.components_.T,
        columns=[f'PC{i+1}' for i in range(8)],
        index=feature_names
    )

    # Print explained variance for each component
    print("\nExplained Variance Ratio:")
    for i, var in enumerate(pca.explained_variance_ratio_):
        print(f"PC{i+1}: {var:.4f} ({var*100:.2f}%)")
    print(f"Cumulative: {np.sum(pca.explained_variance_ratio_):.4f} ({np.sum(pca.explained_variance_ratio_)*100:.2f}%)")

    # Print top contributing features for each PC
    print("\nTop 10 Contributing Features for each Principal Component:")
    for pc in range(8):
        print(f"\nPC{pc+1} Top Features:")
        top_features = pca_loadings[f'PC{pc+1}'].abs().sort_values(ascending=False).head(10)
        print(top_features)
        print(f"Total features contributing to PC{pc+1}: {np.sum(pca_loadings[f'PC{pc+1}'].abs() > 0.1)}")

    print("\n" + "-"*80 + "\n")

# Define all datasets to analyze
datasets = {
    'ALL': profile_train,
    'DRUG': profile_subs,
    'HEALTH': profile_hlth,
    'DEMO': profile_demo
}

for name, data in datasets.items():
    print(f"\n{'='*50}")
    print(f"Variance Analysis for {name} Dataset")
    print(f"{'='*50}")

    # Apply PCA
    pca = PCA()  # No fixed number of components
    pca.fit(data.values)

    # Calculate cumulative variance
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

    # Print variance explained by each component
    print("\nVariance Explained by Components:")
    for i, var in enumerate(pca.explained_variance_ratio_[:10]):  # Show first 10 components
        print(f"PC{i+1}: {var:.4f} ({var*100:.2f}%)")
        print(f"Cumulative: {cumulative_variance[i]:.4f} ({cumulative_variance[i]*100:.2f}%)")

    # Find number of components for different thresholds
    thresholds = [0.7, 0.8, 0.9, 0.95]
    for threshold in thresholds:
        n_components = np.argmax(cumulative_variance >= threshold) + 1
        print(f"\nComponents needed for {threshold*100}% variance: {n_components}")

    # Optional: Plot scree plot
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(pca.explained_variance_ratio_[:20]) + 1),
             pca.explained_variance_ratio_[:20], 'bo-')
    plt.title(f'Scree Plot - {name} Dataset')
    plt.xlabel('Principal Component')
    plt.ylabel('Proportion of Variance Explained')
    plt.show()

#code to save the models

# Initialize dictionaries to store models
pca_models = {}
kmeans_models = {}

# Define all datasets to analyze
datasets = {
    'ALL': profile_train,
    'DRUG': profile_subs,
    'HEALTH': profile_hlth,
    'DEMO': profile_demo
}

for name, data in datasets.items():
    # Apply PCA with 8 components
    pca = PCA(n_components=8)
    pca.fit(data.values)

    # Store PCA model
    pca_models[name] = pca

    # Apply KMeans with 5 clusters
    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(pca.transform(data.values))

    # Store KMeans model
    kmeans_models[name] = kmeans

# Create directory and save models
import os
os.makedirs('saved_models', exist_ok=True)

models_dict = {
    'pca_models': pca_models,
    'kmeans_models': kmeans_models
}

# Save models
with open('saved_models/pca_kmeans_models.pkl', 'wb') as f:
    pickle.dump(models_dict, f)

# Function to load models later
def load_models(filepath='saved_models/pca_kmeans_models.pkl'):
    with open(filepath, 'rb') as f:
        models = pickle.load(f)
    return models['pca_models'], models['kmeans_models']

# Load models
pca_models, kmeans_models = load_models()

# Verify parameters
print("PCA components:", pca_models['ALL'].n_components_)  # Should be 8
print("KMeans clusters:", kmeans_models['ALL'].n_clusters)  # Should be 5

# Load the saved models
pca_models, kmeans_models = load_models()

# For each dataset
datasets_names = ['ALL', 'DRUG', 'HEALTH', 'DEMO']
feature_sets = {
    'ALL': profile_train.columns,
    'DRUG': profile_subs.columns,
    'HEALTH': profile_hlth.columns,
    'DEMO': profile_demo.columns
}

for name in datasets_names:
    print(f"\n{'='*50}")
    print(f"Top Features in Principal Components for {name} Dataset")
    print(f"{'='*50}")

    pca = pca_models[name]
    features = feature_sets[name]

    # Get the absolute value of components
    components = np.abs(pca.components_)

    # For each PC (up to 8)
    for pc in range(8):
        # Get the indices of top 10 features
        top_indices = np.argsort(components[pc])[::-1][:10]

        print(f"\nPC{pc+1} Top Contributing Features:")
        for idx in top_indices:
            # Print feature name and its contribution
            contribution = components[pc][idx]
            print(f"{features[idx]}: {contribution:.4f}")
        print("-" * 30)